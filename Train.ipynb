{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna version: 3.2.0\n",
      "Torch version: 2.0.0+cu117\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats={'png','retina'}\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model import model_utils\n",
    "from model import fusion_model\n",
    "from model import schedular\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Optuna version: {optuna.__version__}')\n",
    "print(f'Torch version: {torch.__version__}')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search\n",
    "+ Adjusting hyperparameters will take a lot of time\n",
    "+ The hyperparameters used for CycPeptMP are the results of 150 trials of search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use auxiliary loss during training\n",
    "USE_AUXILIARY = True\n",
    "\n",
    "def create_model(trial):\n",
    "    \"\"\"\n",
    "    Hyperparameters search for the Fusion model.\n",
    "    \"\"\"\n",
    "    activation_name           = trial.suggest_categorical(\"activation_name\", ['ReLU', 'LeakyReLU', 'SiLU', 'GELU'])\n",
    "    dim_linear                = trial.suggest_categorical(\"dim_linear\", [64, 128, 256, 512])\n",
    "    dim_out                   = trial.suggest_categorical(\"dim_out\", [16, 32, 64])\n",
    "    # Trans\n",
    "    Trans_activation          = activation_name\n",
    "    Trans_dropout_rate        = trial.suggest_float(\"Trans_dropout_rate\", 0.0, 0.3, step=0.05)\n",
    "    Trans_n_encoders          = trial.suggest_int(\"Trans_n_encoders\", 1, 6, 1)\n",
    "    Trans_head_num            = trial.suggest_categorical(\"Trans_head_num\", [4, 8, 16, 32])\n",
    "    Trans_model_dim           = trial.suggest_categorical(\"Trans_model_dim\", [32, 64, 128, 256])\n",
    "    Trans_dim_feedforward     = trial.suggest_categorical(\"Trans_dim_feedforward\", [64, 128, 256, 512])\n",
    "    Trans_gamma_g             = trial.suggest_float(\"Trans_gamma_g\", 0.1, 0.9, step=0.1)\n",
    "    Trans_gamma_c             = 1.0 - Trans_gamma_g\n",
    "    Trans_n_linears           = trial.suggest_int(\"Trans_n_linears\", 1, 2, 1)\n",
    "    Trans_dim_linear          = dim_linear\n",
    "    Trans_dim_out             = dim_out\n",
    "    # CNN\n",
    "    CNN_type                  = trial.suggest_categorical(\"CNN_type\", ['AugCNN', 'AugCyclicConv'])\n",
    "    CNN_num_conv              = trial.suggest_int(\"CNN_num_conv\", 1, 6, 1)\n",
    "    CNN_conv_units            = [int(trial.suggest_categorical(\"conv_units\" + str(i), [32, 64, 128, 256])) for i in range(CNN_num_conv)]\n",
    "    if CNN_type == 'AugCyclicConv':\n",
    "        CNN_padding = 0\n",
    "    elif CNN_type == 'AugCNN':\n",
    "        CNN_padding = 1\n",
    "    CNN_num_linear            = trial.suggest_int(\"CNN_num_linear\", 1, 2, 1)\n",
    "    CNN_linear_units          = [dim_linear]*CNN_num_linear\n",
    "    CNN_activation_name       = activation_name\n",
    "    CNN_pooling_name          = trial.suggest_categorical(\"CNN_pooling_name\", ['max', 'ave'])\n",
    "    CNN_dim_out               = dim_out\n",
    "    # MLP\n",
    "    MLP_num_mlp               = trial.suggest_int(\"MLP_num_mlp\", 1, 6, 1)\n",
    "    MLP_dim_mlp               = trial.suggest_categorical(\"MLP_dim_mlp\", [64, 128, 256, 512])\n",
    "    MLP_dim_linear            = dim_linear\n",
    "    MLP_activation_name       = activation_name\n",
    "    MLP_dropout_rate          = trial.suggest_float(\"MLP_dropout_rate\", 0.0, 0.3, step=0.05)\n",
    "    MLP_dim_out               = dim_out\n",
    "    # concat\n",
    "    Fusion_num_concat         = trial.suggest_int(\"Fusion_num_concat\", 1, 3, 1)\n",
    "    Fusion_concat_units       = [dim_linear]*Fusion_num_concat\n",
    "\n",
    "    model = fusion_model.FusionModel(\n",
    "        DEVICE, USE_AUXILIARY,\n",
    "        # Transformer\n",
    "        Trans_activation, Trans_dropout_rate,\n",
    "        Trans_n_encoders, Trans_head_num, Trans_model_dim, Trans_dim_feedforward,\n",
    "        Trans_gamma_g, Trans_gamma_c,\n",
    "        Trans_n_linears, Trans_dim_linear, Trans_dim_out,\n",
    "        # CNN\n",
    "        CNN_type, CNN_num_conv, CNN_conv_units, CNN_padding,\n",
    "        CNN_activation_name, CNN_pooling_name,\n",
    "        CNN_num_linear, CNN_linear_units, CNN_dim_out,\n",
    "        # MLP\n",
    "        MLP_num_mlp, MLP_dim_mlp,\n",
    "        MLP_activation_name, MLP_dropout_rate,\n",
    "        MLP_dim_linear, MLP_dim_out,\n",
    "        # Fusion\n",
    "        Fusion_num_concat, Fusion_concat_units,\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_optimizer(trial, model):\n",
    "    \"\"\"\n",
    "    Hyperparameters search for the optimizer.\n",
    "    \"\"\"\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer_name\", ['AdamW', 'NAdam', 'RAdam'])\n",
    "    weight_decay = trial.suggest_categorical(\"weight_decay\", [5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1])\n",
    "\n",
    "    # lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01,\n",
    "    if optimizer_name == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), weight_decay=weight_decay)\n",
    "    # lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, momentum_decay=0.004, *, foreach=None, differentiable=False\n",
    "    elif optimizer_name == 'NAdam':\n",
    "        optimizer = torch.optim.NAdam(model.parameters(), weight_decay=weight_decay)\n",
    "    # lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, *, foreach=None, differentiable=False\n",
    "    elif optimizer_name == 'RAdam':\n",
    "        optimizer = torch.optim.RAdam(model.parameters(), weight_decay=weight_decay)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'Fusion'\n",
    "REPLICA_NUM = 60 # Augmentation times\n",
    "\n",
    "EPOCH_NUM = 50\n",
    "PATIENCE = 5 # Stop early when validation loss does not decrease for five consecutive epochs\n",
    "\n",
    "CV = 3\n",
    "\n",
    "gamma_layer  = 0.05 # Weight of auxiliary layer loss\n",
    "gamma_subout = 0.10 # Weight of auxiliary sub-model loss\n",
    "\n",
    "# OPTIMIZE\n",
    "# seed = 2024\n",
    "# model_utils.set_seed(seed)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    time_start_trial = time.time()\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    loss_trial = 0\n",
    "\n",
    "    for cv in range(CV):\n",
    "        folder_path = 'model/input/'\n",
    "        dataset_train = model_utils.load_dataset(folder_path, MODEL_TYPE, REPLICA_NUM, f'Train_{cv}')\n",
    "        dataset_valid = model_utils.load_dataset(folder_path, MODEL_TYPE, REPLICA_NUM, f'Valid_{cv}')\n",
    "\n",
    "        dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "        dataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = create_model(trial)\n",
    "        model = nn.DataParallel(model)\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = create_optimizer(trial, model)\n",
    "        # OPTIMIZE: Adjusting the learning rate\n",
    "        init_lr=0.0001\n",
    "        scheduler = schedular.NoamLR(optimizer=optimizer,\n",
    "                                     warmup_epochs=[0.2*EPOCH_NUM],\n",
    "                                     total_epochs=[EPOCH_NUM],\n",
    "                                     steps_per_epoch=len(dataset_train) // batch_size,\n",
    "                                     init_lr=[init_lr],\n",
    "                                     max_lr=[init_lr*10],\n",
    "                                     final_lr=[init_lr/10])\n",
    "\n",
    "        model_path = f'weight/{MODEL_TYPE}_optuna/{MODEL_TYPE}-{REPLICA_NUM}_t{trial.number}_cv{cv}.cpt'\n",
    "\n",
    "        loss_train_list, loss_valid_list = model_utils.train_loop(model_path, DEVICE, PATIENCE, EPOCH_NUM,\n",
    "                                                                  dataloader_train, dataloader_valid, model, criterion,\n",
    "                                                                  optimizer, scheduler,\n",
    "                                                                  verbose=True,\n",
    "                                                                  use_auxiliary=USE_AUXILIARY, gamma_layer=gamma_layer, gamma_subout=gamma_subout)\n",
    "        # Save complete loss after early stopping\n",
    "        if DEVICE == 'cuda':\n",
    "            checkpoint = torch.load(model_path)\n",
    "        else:\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        checkpoint['loss_train_list'] = loss_train_list\n",
    "        checkpoint['loss_valid_list'] = loss_valid_list\n",
    "        torch.save(checkpoint, model_path)\n",
    "\n",
    "        loss_trial += min(loss_valid_list)\n",
    "\n",
    "\n",
    "    time_end_trial = time.time()\n",
    "    print(f'Execution time of trial {trial.number:03d}: {(time_end_trial-time_start_trial):.0f}')\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "\n",
    "    return loss_trial / CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize', study_name=f'{MODEL_TYPE}-{REPLICA_NUM}',\n",
    "                            load_if_exists=True,\n",
    "                            storage=f'sqlite:///weight/{MODEL_TYPE}_optuna/{MODEL_TYPE}-{REPLICA_NUM}.db')\n",
    "study.optimize(objective, 10)\n",
    "study.trials_dataframe().to_csv(f'weight/{MODEL_TYPE}_optuna/study_history_{MODEL_TYPE}-{REPLICA_NUM}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.load_study(study_name=f'{MODEL_TYPE}-{REPLICA_NUM}',\n",
    "                          storage=f'sqlite:///weight/{MODEL_TYPE}_optuna/{MODEL_TYPE}-{REPLICA_NUM}.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use determined hyperparameters for re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'Fusion'\n",
    "REPLICA_NUM = 60 # Augmentation times\n",
    "\n",
    "EPOCH_NUM = 50\n",
    "PATIENCE = 5 # Stop early when validation loss does not decrease for five consecutive epochs\n",
    "\n",
    "CV = 3\n",
    "\n",
    "gamma_layer  = 0.05 # Weight of auxiliary layer loss\n",
    "gamma_subout = 0.10 # Weight of auxiliary sub-model loss\n",
    "\n",
    "# Use auxiliary loss during training\n",
    "USE_AUXILIARY = True\n",
    "\n",
    "# OPTIMIZE\n",
    "# seed = 2024\n",
    "# model_utils.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results of the hyperparameters search\n",
    "study_history = pd.read_csv(f'weight/{MODEL_TYPE}_optuna/study_history_{MODEL_TYPE}-{REPLICA_NUM}.csv').iloc[:,1:]\n",
    "study_history = study_history[study_history['state'] == 'COMPLETE']\n",
    "best_trial = study_history.sort_values('value').iloc[0]\n",
    "\n",
    "# # Use CycPeptMP hyperparameters\n",
    "# config_path = 'config/CycPeptMP.json'\n",
    "# config = json.load(open(config_path,'r'))\n",
    "# best_trial = config['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(best_trial['params_batch_size'])\n",
    "\n",
    "for cv in range(CV):\n",
    "    folder_path = 'model/input/'\n",
    "    dataset_train = model_utils.load_dataset(folder_path, MODEL_TYPE, REPLICA_NUM, f'Train_{cv}')\n",
    "    dataset_valid = model_utils.load_dataset(folder_path, MODEL_TYPE, REPLICA_NUM, f'Valid_{cv}')\n",
    "\n",
    "    dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    dataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = model_utils.create_model(best_trial, DEVICE, USE_AUXILIARY)\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = model_utils.create_optimizer(best_trial, model)\n",
    "\n",
    "    # OPTIMIZE: Adjusting the learning rate\n",
    "    init_lr=0.0001\n",
    "    scheduler = schedular.NoamLR(optimizer=optimizer,\n",
    "                                 warmup_epochs=[0.2*EPOCH_NUM],\n",
    "                                 total_epochs=[EPOCH_NUM],\n",
    "                                 steps_per_epoch=len(dataset_train) // batch_size,\n",
    "                                 init_lr=[init_lr],\n",
    "                                 max_lr=[init_lr*10],\n",
    "                                 final_lr=[init_lr/10])\n",
    "    # OPTIMIZE\n",
    "    model_path = f'weight/{MODEL_TYPE}_retrain/{MODEL_TYPE}-{REPLICA_NUM}_cv{cv}.cpt'\n",
    "\n",
    "    loss_train_list, loss_valid_list = model_utils.train_loop(model_path, DEVICE, PATIENCE, EPOCH_NUM,\n",
    "                                                              dataloader_train, dataloader_valid, model, criterion,\n",
    "                                                              optimizer, scheduler,\n",
    "                                                              verbose=True,\n",
    "                                                              use_auxiliary=USE_AUXILIARY, gamma_layer=gamma_layer, gamma_subout=gamma_subout)\n",
    "    # Save complete loss after early stopping\n",
    "    if DEVICE == 'cuda':\n",
    "        checkpoint = torch.load(model_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    checkpoint['loss_train_list'] = loss_train_list\n",
    "    checkpoint['loss_valid_list'] = loss_valid_list\n",
    "    torch.save(checkpoint, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
